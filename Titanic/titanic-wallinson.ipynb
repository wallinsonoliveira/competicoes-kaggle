{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":3136,"databundleVersionId":26502,"sourceType":"competition"}],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Kaggle","metadata":{}},{"cell_type":"markdown","source":"## <p style=\"background-color:#d8ecff; color: #009dff;margin:0; display:inline-block;padding:.4rem;border-radius:.25rem;border:1px solid #009dff\">Saving Submission file Using Best Model</p>\n\n\n### <p style=\"background-color: #fdefff;color:#c12eff;margin: 0;display: inline-block;padding:.4rem;border-radius:.5rem;border: 1px solid #c059ff\">Please Upvote if you Really liked this</p>","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-11-05T14:50:59.864041Z","iopub.execute_input":"2024-11-05T14:50:59.864557Z","iopub.status.idle":"2024-11-05T14:51:01.107980Z","shell.execute_reply.started":"2024-11-05T14:50:59.864502Z","shell.execute_reply":"2024-11-05T14:51:01.106667Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/titanic/train.csv\n/kaggle/input/titanic/test.csv\n/kaggle/input/titanic/gender_submission.csv\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Bibliotecas","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings('ignore')\n\nfrom sklearn.metrics import accuracy_score, precision_score,roc_auc_score,confusion_matrix, classification_report","metadata":{"execution":{"iopub.status.busy":"2024-11-05T14:51:01.110385Z","iopub.execute_input":"2024-11-05T14:51:01.111076Z","iopub.status.idle":"2024-11-05T14:51:03.187905Z","shell.execute_reply.started":"2024-11-05T14:51:01.111017Z","shell.execute_reply":"2024-11-05T14:51:03.186666Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"## Dados","metadata":{}},{"cell_type":"code","source":"train = pd.read_csv(\"../input/titanic/train.csv\")\ntest = pd.read_csv(\"../input/titanic/test.csv\")\nexemplo = pd.read_csv(\"../input/titanic/gender_submission.csv\")\nrespostacorreta = pd.read_csv('https://raw.githubusercontent.com/wallinsonoliveira/competicoes-kaggle/refs/heads/main/Titanic/respostacorreta.csv', na_values='?')\ntest[\"Survived\"] = respostacorreta[\"Survived\"]\n#print(train.shape)\n#print(test.shape)\n#test.head()\n\n#Train - PassengerId: 1 a 891\n#Test - PassengerId: 892 a 1309","metadata":{"execution":{"iopub.status.busy":"2024-11-04T20:03:44.005784Z","iopub.execute_input":"2024-11-04T20:03:44.006162Z","iopub.status.idle":"2024-11-04T20:03:44.062524Z","shell.execute_reply.started":"2024-11-04T20:03:44.006128Z","shell.execute_reply":"2024-11-04T20:03:44.061472Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Análise de Dados 1","metadata":{}},{"cell_type":"code","source":"train","metadata":{"execution":{"iopub.status.busy":"2024-11-04T20:03:44.064450Z","iopub.execute_input":"2024-11-04T20:03:44.065371Z","iopub.status.idle":"2024-11-04T20:03:44.085014Z","shell.execute_reply.started":"2024-11-04T20:03:44.065319Z","shell.execute_reply":"2024-11-04T20:03:44.083724Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2024-11-04T20:03:44.086181Z","iopub.execute_input":"2024-11-04T20:03:44.086557Z","iopub.status.idle":"2024-11-04T20:03:44.099668Z","shell.execute_reply.started":"2024-11-04T20:03:44.086519Z","shell.execute_reply":"2024-11-04T20:03:44.098649Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.set_style('whitegrid')\nsns.countplot(x='Survived',data=train)","metadata":{"execution":{"iopub.status.busy":"2024-11-04T20:03:44.102630Z","iopub.execute_input":"2024-11-04T20:03:44.103099Z","iopub.status.idle":"2024-11-04T20:03:44.297942Z","shell.execute_reply.started":"2024-11-04T20:03:44.103049Z","shell.execute_reply":"2024-11-04T20:03:44.296876Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"filtered_data = train[~train['Embarked'].isin(['C', 'S', 'Q'])]\nfiltered_data","metadata":{"execution":{"iopub.status.busy":"2024-11-04T20:03:44.299311Z","iopub.execute_input":"2024-11-04T20:03:44.299680Z","iopub.status.idle":"2024-11-04T20:03:44.316435Z","shell.execute_reply.started":"2024-11-04T20:03:44.299644Z","shell.execute_reply":"2024-11-04T20:03:44.315331Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.displot(data=train,x='Embarked')","metadata":{"execution":{"iopub.status.busy":"2024-11-04T20:03:44.317995Z","iopub.execute_input":"2024-11-04T20:03:44.318355Z","iopub.status.idle":"2024-11-04T20:03:44.663494Z","shell.execute_reply.started":"2024-11-04T20:03:44.318316Z","shell.execute_reply":"2024-11-04T20:03:44.662448Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.countplot(x='Survived',data=train,hue='Sex',palette='RdBu_r')","metadata":{"execution":{"iopub.status.busy":"2024-11-04T20:03:44.664805Z","iopub.execute_input":"2024-11-04T20:03:44.665134Z","iopub.status.idle":"2024-11-04T20:03:44.888215Z","shell.execute_reply.started":"2024-11-04T20:03:44.665100Z","shell.execute_reply":"2024-11-04T20:03:44.887205Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.countplot(x='Survived',data=train,hue='Pclass')","metadata":{"execution":{"iopub.status.busy":"2024-11-04T20:03:44.889485Z","iopub.execute_input":"2024-11-04T20:03:44.889830Z","iopub.status.idle":"2024-11-04T20:03:45.140510Z","shell.execute_reply.started":"2024-11-04T20:03:44.889795Z","shell.execute_reply":"2024-11-04T20:03:45.139353Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.displot(train['Age'],kde=False,bins=30)","metadata":{"execution":{"iopub.status.busy":"2024-11-04T20:03:45.142056Z","iopub.execute_input":"2024-11-04T20:03:45.142449Z","iopub.status.idle":"2024-11-04T20:03:45.699551Z","shell.execute_reply.started":"2024-11-04T20:03:45.142398Z","shell.execute_reply":"2024-11-04T20:03:45.698473Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"survived_fares = train[train['Survived'] == 1]['Fare']\nnot_survived_fares = train[train['Survived'] == 0]['Fare']\n\nplt.figure(figsize=(10, 6))\nplt.hist([survived_fares, not_survived_fares], bins=40, color=['green', 'red'], label=['Survived', 'Not Survived'])\nplt.xlabel('Fare')\nplt.ylabel('Frequencia')\nplt.title('Distribuição de Taxas por Sobreviventes')\nplt.legend()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-11-04T20:03:45.703300Z","iopub.execute_input":"2024-11-04T20:03:45.703656Z","iopub.status.idle":"2024-11-04T20:03:46.155599Z","shell.execute_reply.started":"2024-11-04T20:03:45.703621Z","shell.execute_reply":"2024-11-04T20:03:46.154476Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.countplot(x='SibSp',data=train,hue='Survived',palette='RdBu_r')","metadata":{"execution":{"iopub.status.busy":"2024-11-04T20:03:46.156952Z","iopub.execute_input":"2024-11-04T20:03:46.157391Z","iopub.status.idle":"2024-11-04T20:03:46.490954Z","shell.execute_reply.started":"2024-11-04T20:03:46.157346Z","shell.execute_reply":"2024-11-04T20:03:46.489927Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.countplot(x='Parch',data=train,hue='Survived',palette='RdBu_r')","metadata":{"execution":{"iopub.status.busy":"2024-11-04T20:03:46.492373Z","iopub.execute_input":"2024-11-04T20:03:46.492843Z","iopub.status.idle":"2024-11-04T20:03:46.808213Z","shell.execute_reply.started":"2024-11-04T20:03:46.492793Z","shell.execute_reply":"2024-11-04T20:03:46.806883Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.heatmap(train.isnull(),yticklabels=False,cmap=('viridis'))","metadata":{"execution":{"iopub.status.busy":"2024-11-04T20:03:46.809499Z","iopub.execute_input":"2024-11-04T20:03:46.809862Z","iopub.status.idle":"2024-11-04T20:03:47.247681Z","shell.execute_reply.started":"2024-11-04T20:03:46.809826Z","shell.execute_reply":"2024-11-04T20:03:47.246388Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.heatmap(test.isnull(),yticklabels=False,cmap=('viridis'))","metadata":{"execution":{"iopub.status.busy":"2024-11-04T20:03:47.249626Z","iopub.execute_input":"2024-11-04T20:03:47.249996Z","iopub.status.idle":"2024-11-04T20:03:47.675858Z","shell.execute_reply.started":"2024-11-04T20:03:47.249959Z","shell.execute_reply":"2024-11-04T20:03:47.674818Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Tratamento de Dados","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\n\n# Copiar dados de treino e teste - original# Copiar dados de treino e teste\ndados_train = train.copy()\ndados_test = test.copy()\n\n# Concatenar dados de treino e teste para processar ambos de uma vez\nconcat = pd.concat([dados_train, dados_test], axis=0, ignore_index=True)\n\n# Correção de erros na base\nconcat['Parch'] = np.where(concat['Name'].isin(['Newell, Miss. Madeleine', 'Newell, Miss. Marjorie', 'Ware, Mrs. John James (Florence Louise Long)', 'Ware, Mr. John James', 'Watt, Mrs. James (Elizabeth \"Bessie\" Inglis Milne)', 'Watt, Miss. Bertha J']), 1, concat['Parch'])\nconcat['Parch'] = np.where(concat['Name']=='Natsch, Mr. Charles H', 0, concat['Parch'])\nconcat['SibSp'] = np.where(concat['Name'].isin(['Risien, Mrs. Samuel (Emma)', 'Risien, Mr. Samuel Beard']), 1, concat['SibSp'])\n\n\n#concat['Sobrenome'] = concat['Name'].apply(lambda x: x.split(',')[0].strip())\n#sobrenomes_unicos = concat['Sobrenome'].unique()\n#categorias = {sobrenome: chr(65 + i) for i, sobrenome in enumerate(sobrenomes_unicos)}\n#concat['CategoriaSobrenome'] = concat['Sobrenome'].map(categorias)\n\n# Identifica a \"moda\" (valor mais comum) da coluna `Embarked`\nembarked_mode = concat['Embarked'].mode()[0]\n\n# Preenche valoes NaN da coluna `Embarked com a \"moda\"\nconcat['Embarked'] = concat['Embarked'].fillna(embarked_mode)\n\n# Cria uma coluna T_partner com a soma dos familiares a bordo\nconcat['T_partner'] = concat[\"SibSp\"]+train[\"Parch\"]\n\n# Cria uma coluna Alone que indica se o passageiro estava sozinho ou não\nconcat['Alone'] = np.where(concat['T_partner']>0, 0, 1)\n\n# Extrai do nome o título do passageiro e depois converte para números de acordo o título\nconcat['Words_Count'] = concat['Name'].apply(lambda x: len(x.split()))\nconcat['Title'] = concat.Name.str.extract(' ([A-Za-z]+)\\.', expand=False)\nconcat['Title'] = concat['Title'].replace(['Lady', 'Countess','Capt', 'Col', 'Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\nconcat['Title'] = concat['Title'].replace('Mlle', 'Miss')\nconcat['Title'] = concat['Title'].replace('Ms', 'Miss')\nconcat['Title'] = concat['Title'].replace('Mme', 'Mrs')\ntitle_mapping = {\"Mr\": 1, \"Miss\": 2, \"Mrs\": 3, \"Master\": 4, \"Rare\": 5}\nconcat['Title'] = concat['Title'].map(title_mapping)\nconcat['Title'] = concat['Title'].fillna(0)\n\n#Preencher Age nulas com a média das idades - Tentar preencher com a média por sexo também\n#concat['Age_Was_Null'] = concat['Age'].isnull()  # Identificar valores nulos antes do preenchimento\n#concat['Age'] = pd.to_numeric(concat['Age'], errors='coerce')\n#concat['Age'] = concat['Age'].fillna(concat['Age'].median())\n\n# Preenche idades nulas com a média das idades por `Title`\nmean_age_per_title = concat.groupby('Title')['Age'].transform('mean')\nconcat['Age'] = concat['Age'].fillna(mean_age_per_title)\n\n# Substitui dados nulos da cabine por U (Unknown)\nconcat['Cabin'] = concat['Cabin'].fillna('U')\n\nimport re\n# Extrai primeira letra da cabine\nconcat['Cabin'] = concat['Cabin'].map(lambda x: re.compile(\"([a-zA-Z]+)\").search(x).group())\ncabin_category = {'A':9, 'B':8, 'C':7, 'D':6, 'E':5, 'F':4, 'G':3, 'T':2, 'U':1}\n\n# Substitui letras por numeros de acordo com a categoria, onde U é desconhecido\nconcat['Cabin'] = concat['Cabin'].map(cabin_category)\n\n# Cria uma coluna is_minor com 1 se o passageiro tiver menos de 16 anos\nconcat['is_minor']=np.where(concat['Age']<=16,1,0)\n\n# Limpa tickets. Pega os 3 primeiros dígitos do ticket, converte para categorias e depois para números\nconcat['Ticket_type'] = concat['Ticket'].apply(lambda x: x[0:3])\nconcat['Ticket_type'] = concat['Ticket_type'].astype('category')\nconcat['Ticket_type'] = concat['Ticket_type'].cat.codes\n\n# Preencher valores nulos da coluna `Fare` com a média da coluna `Fare` dos registros da classe relacionada à linha que a coluna `Fare` está NaN\nmean_fare_by_pclass = concat.groupby('Pclass')['Fare'].transform('mean')\nconcat['Fare'] = concat['Fare'].fillna(mean_fare_by_pclass)\n\n# Apagar colunas extras\nconcat.drop([\"T_partner\", \"Parch\", \"SibSp\", \"Name\", \"Ticket\",\"Words_Count\"], axis = 1, inplace = True)\n\n# Transformar colunas categórias em numérias usando get_dummies\nconcat = pd.get_dummies(concat, drop_first=True)\n\n# Filtrar a linha onde PassengerID é igual a 1044\n#linha_1044 = concat[concat['PassengerId'] == 1044]\n#print(linha_1044)\n\n### BASE TREINAMENTO\n#df = concat[concat['Survived'].notnull()].copy()\ndf = concat[concat['PassengerId'].between(1, 891)]\ndf = df.drop(columns=['PassengerId'])\ndf['Survived'] = df['Survived'].astype(int)\n\n#Train - PassengerId: 1 a 891\n#Test - PassengerId: 892 a 1309\n\n### BASE TESTE - PARA PREVISÃO E SUBMISSÃO\n#teste = concat[concat['Survived'].isnull()].copy()\nteste = concat[concat['PassengerId'].between(892, 1309)]\nteste_modelo_y = teste[\"Survived\"]\nteste_modelo = teste.drop(columns=['PassengerId', 'Survived']) # para ser usado no modelo\nteste_previsao = teste.drop(columns=['Survived']) # para ser usado para submissão\n\nprint(\"feito\")","metadata":{"execution":{"iopub.status.busy":"2024-11-04T20:03:47.677526Z","iopub.execute_input":"2024-11-04T20:03:47.677960Z","iopub.status.idle":"2024-11-04T20:03:47.735758Z","shell.execute_reply.started":"2024-11-04T20:03:47.677912Z","shell.execute_reply":"2024-11-04T20:03:47.734858Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#concat.isnull().sum()\n#concat.info()\nconcat.head()\n#teste_modelo.info()\n#teste_modelo_y.info()\n#teste_previsao.info()\n#res_y.info()\n#print(concat.head(10))","metadata":{"execution":{"iopub.status.busy":"2024-11-04T20:03:47.736969Z","iopub.execute_input":"2024-11-04T20:03:47.737310Z","iopub.status.idle":"2024-11-04T20:03:47.754436Z","shell.execute_reply.started":"2024-11-04T20:03:47.737275Z","shell.execute_reply":"2024-11-04T20:03:47.753468Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Análise de Dados 2","metadata":{}},{"cell_type":"code","source":"sns.heatmap(concat.isnull(),yticklabels=False,cmap=('viridis'))","metadata":{"execution":{"iopub.status.busy":"2024-11-04T20:03:47.755834Z","iopub.execute_input":"2024-11-04T20:03:47.756268Z","iopub.status.idle":"2024-11-04T20:03:48.252898Z","shell.execute_reply.started":"2024-11-04T20:03:47.756221Z","shell.execute_reply":"2024-11-04T20:03:48.251753Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"\n#sns.countplot(x='Title',data=concat,hue='Age',palette='RdBu_r')\nmean_age_per_title = concat.groupby('Title')['Age'].mean().reset_index()\n\n# Plota a média de idade por título\nplt.figure(figsize=(10, 6))\nsns.barplot(x='Title', y='Age', data=mean_age_per_title, palette='RdBu_r')\nplt.ylabel('Média de Idade')\nplt.title('Média de Idade por Título')\nplt.show()\n\"\"\"","metadata":{"execution":{"iopub.status.busy":"2024-11-04T20:03:48.254285Z","iopub.execute_input":"2024-11-04T20:03:48.254764Z","iopub.status.idle":"2024-11-04T20:03:48.267474Z","shell.execute_reply.started":"2024-11-04T20:03:48.254720Z","shell.execute_reply":"2024-11-04T20:03:48.266186Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(30, 3))\ncorr_matrix = concat.drop(columns=['PassengerId']).corr()\nsns.heatmap(corr_matrix.iloc[0:1], annot=True, cmap=\"coolwarm\")\nplt.show()\n#corr_matrix","metadata":{"execution":{"iopub.status.busy":"2024-11-04T20:03:48.268714Z","iopub.execute_input":"2024-11-04T20:03:48.269072Z","iopub.status.idle":"2024-11-04T20:03:49.072448Z","shell.execute_reply.started":"2024-11-04T20:03:48.269036Z","shell.execute_reply":"2024-11-04T20:03:49.071384Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Configura o tamanho da figura\nplt.figure(figsize=(15, 10))\n\n# Calcula e plota a matriz de correlação completa, ignorando a coluna 'PassengerId'\ncorr_matrix = concat.drop(columns=['PassengerId']).corr()\nsns.heatmap(corr_matrix, annot=True, cmap=\"coolwarm\", vmin=-1, vmax=1)\n\n# Exibe o gráfico\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2024-11-04T20:03:49.073748Z","iopub.execute_input":"2024-11-04T20:03:49.074105Z","iopub.status.idle":"2024-11-04T20:03:49.892561Z","shell.execute_reply.started":"2024-11-04T20:03:49.074068Z","shell.execute_reply":"2024-11-04T20:03:49.891600Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Pre-processamento dos Dados","metadata":{}},{"cell_type":"code","source":"#train_res = train[\"Survived\"].reset_index(drop = True)\n\n# Separa a coluna alvo 'Survived' em y\ny = df['Survived']\n\n# Separa outras colunas em X\nX = df.drop(columns=['Survived'])\n\n# Dividir dataset em treino e teste\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42, stratify=y)\nprint(\"feito\")","metadata":{"execution":{"iopub.status.busy":"2024-11-04T20:03:49.893927Z","iopub.execute_input":"2024-11-04T20:03:49.894382Z","iopub.status.idle":"2024-11-04T20:03:49.906586Z","shell.execute_reply.started":"2024-11-04T20:03:49.894331Z","shell.execute_reply":"2024-11-04T20:03:49.905343Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Busca por Hiperparâmetros","metadata":{}},{"cell_type":"markdown","source":"### Busca Bayesiana","metadata":{}},{"cell_type":"code","source":"\"\"\"\n# HyperOpt - Busca Bayesiana\n\n#!pip install hyperopt\n\nfrom hyperopt import hp, tpe, fmin, Trials, STATUS_OK\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.preprocessing import scale, normalize\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom catboost import CatBoostClassifier\nfrom lightgbm import LGBMClassifier\nimport numpy as np\nfrom sklearn.tree import DecisionTreeClassifier\nimport logging\nfrom sklearn.linear_model import LogisticRegression, Perceptron\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier, plot_tree\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.naive_bayes import BernoulliNB\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier, ExtraTreesClassifier, BaggingClassifier, StackingClassifier, VotingClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\nfrom catboost import CatBoostClassifier\nimport catboost as catboost\nimport logging\nfrom sklearn.model_selection import cross_val_score, GridSearchCV\nfrom hyperopt import hp, tpe, fmin, Trials, STATUS_OK\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.ensemble import BaggingClassifier, RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nimport numpy as np\n\nfrom sklearn.metrics import accuracy_score, f1_score, confusion_matrix\nimport pandas as pd\n\n# Modelos - aqui são as classes, não as instâncias já criadas\nmodels = {\n    'logistic_regression': LogisticRegression,\n    'rf': RandomForestClassifier,\n    'knn': KNeighborsClassifier,\n    'svc': SVC,\n    'decision_tree': DecisionTreeClassifier,\n    'xgboost': XGBClassifier,\n    'catboost': CatBoostClassifier,\n    'lgbm': LGBMClassifier,\n    'bagging': BaggingClassifier\n}\n\nbase_X_train = X.copy()\nbase_y_train = y.copy()\n\ndef search_space(model):\n    model = model.lower()\n    space = {}\n    if model == 'bagging':\n        space = {\n            #'n_estimators': hp.choice('n_estimators', range(10, 200)),  # Número de estimadores\n            'max_samples': hp.uniform('max_samples', 0.1, 1.0),  # Proporção do conjunto de dados\n            'max_features': hp.uniform('max_features', 0.1, 1.0),  # Proporção de características\n            'bootstrap': hp.choice('bootstrap', [True, False]),  # Amostragem com reposição\n            'base_estimator': hp.choice('base_estimator', [\n                LogisticRegression(),  # Corrigido: agora passando a instância\n                #RandomForestClassifier(),\n                DecisionTreeClassifier()\n            ])  # Instâncias de modelos de base para o Bagging\n        }\n    elif model == 'catboost':\n        space = {\n            'iterations': hp.choice('iterations', range(100,102)),\n            'depth': hp.choice('depth', range(3, 10)),\n            'learning_rate': hp.uniform('learning_rate', 0.01, 0.3),\n            'l2_leaf_reg': hp.uniform('l2_leaf_reg', 1, 10),\n            'border_count': hp.choice('border_count', range(32, 255))\n        }\n    elif model == 'lgbm':\n        space = {\n            'n_estimators': hp.choice('n_estimators', range(100, 1000)),\n            'max_depth': hp.choice('max_depth', range(3, 15)),\n            'learning_rate': hp.uniform('learning_rate', 0.01, 0.3),\n            'num_leaves': hp.choice('num_leaves', range(20, 150)),\n            'feature_fraction': hp.uniform('feature_fraction', 0.5, 1.0)\n        }\n    elif model == 'knn':\n        space = {\n            'n_neighbors': hp.choice('n_neighbors', range(3, 100)),\n            'p': hp.choice('p', range(1, 20))\n            #'scale': hp.choice('scale', [0, 1]),\n            #'normalize': hp.choice('normalize', [0, 1]),\n        }\n    elif model == 'svc':\n      space = {\n          'C': hp.uniform('C', 0, 1000),\n          #'kernel': hp.choice('kernel', ['linear', 'sigmoid', 'poly', 'rbf']),\n          'gamma': hp.choice('gamma', ['auto', 'scale']),  # Substitua 'RS*' por 'scale'\n          'class_weight': hp.choice('class_weight', ['balanced', None])  # Corrigido de 'kernel' para 'class_weight'\n      }\n\n    elif model == 'logistic_regression':\n        space = {\n            #'fit_intercept': hp.choice('fit_intercept', [True, False]),\n            #'C': hp.uniform('C', 0.05, 3),\n            'C': hp.uniform('C', 0.05, 100),\n            'solver': hp.choice('solver', ['liblinear']),\n            #'solver': hp.choice('solver', ['newton-cg', 'lbfgs', 'liblinear']),\n            #'max_iter': hp.choice('max_iter', range(2, 1000)),\n            #'multi_class': 'auto',\n            #'class_weight': 'balanced'\n            'penalty': hp.choice('penalty',['l1','l2'])\n        }\n\n    elif model == 'rf':\n        space = {\n            'max_depth': hp.choice('max_depth', range(1, 200)),\n            'min_samples_split': hp.choice('min_samples_split', range(2, 40)),\n            'min_samples_leaf': hp.choice('min_samples_leaf', range(1, 20)),\n            'max_features': hp.choice('max_features', range(1, 3)),\n            'n_estimators': hp.choice('n_estimators', range(50, 1200))\n        }\n\n    elif model == 'decision_tree':  # Adicionando Decision Tree\n        space = {\n            'criterion': hp.choice('criterion', ['gini', 'entropy']),\n            'max_depth': hp.choice('max_depth', range(1, 100)),\n            'min_samples_split': hp.choice('min_samples_split', range(2, 100)),\n            'min_samples_leaf': hp.choice('min_samples_leaf', range(1, 20)),\n            'max_features': hp.choice('max_features', ['sqrt', 'log2', None])\n        }\n\n    elif model == 'xgboost':  # Adicionando espaço de busca para o XGBoost\n        space = {\n            'eta': hp.choice('eta', [0.01, 0.015, 0.025, 0.05, 0.1]),\n            'gamma': hp.choice('gamma', [0.05, 0.1, 0.3, 0.5, 0.7, 0.9, 1.0]),\n            'max_depth': hp.choice('max_depth', [3, 5, 7, 9, 12, 15, 17, 25]),\n            'min_child_weight': hp.choice('min_child_weight', [1, 3, 5, 7]),\n            'subsample': hp.choice('subsample', [0.6, 0.7, 0.8, 0.9, 1.0]),\n            'colsample_bytree': hp.choice('colsample_bytree', [0.6, 0.7, 0.8, 0.9, 1.0]),\n            'lambda': hp.choice('lambda', [0.01, 0.1, 1.0]),  # RS* não é suportado diretamente\n            'alpha': hp.choice('alpha', [0, 0.1, 0.5, 1.0])   # RS* removido para evitar erro\n         }\n\n    space['model'] = model\n    return space\n\ndef get_acc_status(clf, X_, y):\n    acc = cross_val_score(clf, X_, y, cv=5).mean()\n    return {'loss': -acc, 'status': STATUS_OK}\n\ndef scale_normalize(params, X_):\n    if params.get('scale', 0):\n        X_ = scale(X_)\n    if params.get('normalize', 0):\n        X_ = normalize(X_)\n    return X_\n\ndef obj_fnc(params):\n    model_name = params.get('model').lower()\n    X_ = scale_normalize(params, base_X_train[:])  # Agora usando X_train\n    del params['model']\n\n    # Instanciar o modelo com os parâmetros ajustados\n    clf = models[model_name](**params)\n\n    return get_acc_status(clf, X_, base_y_train)  # Agora usando y_train\n\nhypopt_trials = Trials()\n\n# Executando a busca bayesiana\nbest_params = fmin(\n    fn=obj_fnc,\n    space=search_space('bagging'),  # Ajuste aqui o modelo desejado ('knn', 'svc', etc.)\n    algo=tpe.suggest,\n    max_evals=1000,\n    trials=hypopt_trials\n)\n\nprint(best_params)\nprint(hypopt_trials.best_trial['result']['loss'])\n\"\"\"","metadata":{"execution":{"iopub.status.busy":"2024-11-04T20:03:49.908591Z","iopub.execute_input":"2024-11-04T20:03:49.908954Z","iopub.status.idle":"2024-11-04T20:03:49.922029Z","shell.execute_reply.started":"2024-11-04T20:03:49.908917Z","shell.execute_reply":"2024-11-04T20:03:49.920969Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Grid Search","metadata":{}},{"cell_type":"code","source":"\"\"\"\n# Grid Search - SVM\nfrom hyperopt import hp, tpe, fmin, Trials, STATUS_OK\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.preprocessing import scale, normalize\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom catboost import CatBoostClassifier\nfrom lightgbm import LGBMClassifier\nimport numpy as np\nfrom sklearn.tree import DecisionTreeClassifier\nimport logging\nfrom sklearn.linear_model import LogisticRegression, Perceptron\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier, plot_tree\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.naive_bayes import BernoulliNB\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier, ExtraTreesClassifier, BaggingClassifier, StackingClassifier, VotingClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\nfrom catboost import CatBoostClassifier\nimport catboost as catboost\nimport logging\nfrom sklearn.model_selection import cross_val_score, GridSearchCV\n\nfrom sklearn.metrics import accuracy_score, f1_score, confusion_matrix\nimport pandas as pd\n\nbase_X_train = X.copy()\nbase_y_train = y.copy()\n\nsvm = SVC(random_state=100)\n\nparam_grid = {\n    'C': list(range(1, 500, 10)),                # 1 a 100 de 5 em 5 passos\n    #'kernel': ['linear', 'rbf', 'poly', 'sigmoid'],             # Função de kernel a ser usada\n    #'degree': [1, 2, 3, 4],                                     # Grau do polinômio se o kernel for 'poly'\n    #'gamma': ['scale', 'auto', 0.01, 0.1, 1],                  # Coeficiente do kernel\n    'coef0': [0.0, 0.5, 1.0],                                   # Termo independente no núcleo\n    'shrinking': [True, False],                                 # Habilita ou desabilita o encolhimento\n    #'class_weight': [None, 'balanced']                         # Peso das classes\n    'tol': [1e-4, 1e-3, 1e-2],                                 # Tolerância para critério de parada\n    #'max_iter': [100, 500]                                 # Número máximo de iterações\n}\n\n#Melhores hiperparâmetros: {'C': 81, 'class_weight': 'balanced', 'degree': 1, 'gamma': 'scale'}\n#F1-Score:  0.71378350410412\n#Melhores hiperparâmetros: {'C': 201, 'class_weight': 'balanced', 'degree': 1, 'gamma': 'scale'}\n#F1-Score:  0.7249733335690588\n\n# Realizar a busca de hiperparâmetros com validação cruzada\ngrid_search = GridSearchCV(svm, param_grid, cv=3, n_jobs=-1, scoring='f1')\ngrid_search.fit(base_X_train, base_y_train)\n\n# Melhor modelo encontrado\nbest_model = grid_search.best_estimator_\n\n# Exibir os melhores hiperparâmetros e o F1-Score\nprint(\"Melhores hiperparâmetros:\", grid_search.best_params_)\nprint(\"F1-Score: \", grid_search.best_score_)\n\"\"\"","metadata":{"execution":{"iopub.status.busy":"2024-11-04T20:03:49.923700Z","iopub.execute_input":"2024-11-04T20:03:49.924069Z","iopub.status.idle":"2024-11-04T20:03:49.938723Z","shell.execute_reply.started":"2024-11-04T20:03:49.924029Z","shell.execute_reply":"2024-11-04T20:03:49.937695Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"\n# KNN\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import GridSearchCV\n\n# Supondo que base_X_train e base_y_train sejam os seus dados de treinamento\nbase_X_train = X.copy()\nbase_y_train = y.copy()\n\n# Inicializando o modelo KNN\nknn = KNeighborsClassifier()\n\n# Grade de parâmetros para otimização\nparam_grid = {\n    'n_neighbors': list(range(1, 50, 5)),        # Número de vizinhos (de 1 a 50 com passo de 5)\n    'weights': ['uniform', 'distance'],          # Peso uniforme ou por distância\n    'metric': ['euclidean', 'manhattan', 'minkowski'],  # Métrica de distância\n    'p': [1, 2]                                  # Parâmetro de potência da métrica Minkowski (1 para manhattan, 2 para euclidean)\n}\n\n# Configuração da busca em grade com validação cruzada\ngrid_search = GridSearchCV(\n    estimator=knn,\n    param_grid=param_grid,\n    scoring='accuracy',       # Critério de avaliação, pode ser ajustado conforme necessário\n    cv=5,                     # Número de folds na validação cruzada\n    verbose=1,                # Nível de detalhamento\n    n_jobs=-1                 # Usa todos os processadores disponíveis\n)\n\n# Realizando a busca\ngrid_search.fit(base_X_train, base_y_train)\n\n# Melhor combinação de parâmetros encontrada\nbest_params = grid_search.best_params_\nbest_score = grid_search.best_score_\n\nprint(\"Melhores parâmetros:\", best_params)\nprint(\"Melhor acurácia:\", best_score)\n\"\"\"","metadata":{"execution":{"iopub.status.busy":"2024-11-04T20:03:49.940223Z","iopub.execute_input":"2024-11-04T20:03:49.940578Z","iopub.status.idle":"2024-11-04T20:03:49.954632Z","shell.execute_reply.started":"2024-11-04T20:03:49.940544Z","shell.execute_reply":"2024-11-04T20:03:49.953484Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"\n# Decison Tree\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import GridSearchCV\n\n# Supondo que base_X_train e base_y_train sejam os seus dados de treinamento\nbase_X_train = X.copy()\nbase_y_train = y.copy()\n\n# Inicializando o modelo de Árvore de Decisão\ndtree = DecisionTreeClassifier()\n\n# Grade de parâmetros para otimização\nparam_grid = {\n    'criterion': ['gini', 'entropy','log_loss'],  # Critério de divisão da árvore\n    'max_depth': [None] + list(range(18, 21)),  # Profundidade máxima da árvore (None para crescimento ilimitado)\n    'min_samples_split': [2] + list(range(5, 15)),  # Número mínimo de amostras necessárias para dividir um nó\n    'min_samples_leaf': [1] + list(range(1, 11)),   # Número mínimo de amostras que um nó folha deve ter\n    'max_features': [0,1,2]  # Número de recursos a considerar ao procurar a melhor divisão\n}\n#    {'criterion': 0, 'max_depth': 19, 'max_features': 2, 'min_samples_leaf': 7, 'min_samples_split': 19}\n\n# Configuração da busca em grade com validação cruzada\ngrid_search = GridSearchCV(\n    estimator=dtree,\n    param_grid=param_grid,\n    scoring='accuracy',       # Critério de avaliação\n    cv=5,                     # Número de folds na validação cruzada\n    verbose=1,                # Nível de detalhamento\n    n_jobs=-1                 # Usa todos os processadores disponíveis\n)\n\n# Realizando a busca\ngrid_search.fit(base_X_train, base_y_train)\n\n# Melhor combinação de parâmetros encontrada\nbest_params = grid_search.best_params_\nbest_score = grid_search.best_score_\n\nprint(\"Melhores parâmetros:\", best_params)\nprint(\"Melhor acurácia:\", best_score)\n\"\"\"","metadata":{"execution":{"iopub.status.busy":"2024-11-04T20:03:49.955932Z","iopub.execute_input":"2024-11-04T20:03:49.956273Z","iopub.status.idle":"2024-11-04T20:03:49.972232Z","shell.execute_reply.started":"2024-11-04T20:03:49.956238Z","shell.execute_reply":"2024-11-04T20:03:49.971122Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Algoritmos - Teste Automático","metadata":{}},{"cell_type":"code","source":"# Algoritmos de aprendizagem\nimport logging\nfrom sklearn.linear_model import LogisticRegression, Perceptron\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier, plot_tree\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.naive_bayes import BernoulliNB\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier, ExtraTreesClassifier, BaggingClassifier, StackingClassifier, VotingClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\nfrom catboost import CatBoostClassifier\nimport catboost as catboost\nimport logging\nfrom sklearn.model_selection import cross_val_score, GridSearchCV\n\nfrom sklearn.metrics import accuracy_score, f1_score, confusion_matrix\nimport pandas as pd\n\n# Configurando o logger do LightGBM\nlgb_logger = logging.getLogger(\"lightgbm\")\nlgb_logger.setLevel(logging.ERROR)\n\nnc = 5\nbase_X_train = X.copy()\nbase_y_train = y.copy()\n#teste_modelo = pd.get_dummies(teste, drop_first=True)\n#teste_modelo = teste_modelo.reindex(columns=X.columns, fill_value=False)\n\nresultados_df = pd.DataFrame()\n\n# Modelos de Machine Learning\n\nestimators = [\n    ('rf', RandomForestClassifier(max_depth=20,max_features=1,min_samples_leaf=1,min_samples_split=4,n_estimators=1010,random_state=42)),\n    ('gb', GradientBoostingClassifier()),\n    ('et', ExtraTreesClassifier()),\n    ('catbost', CatBoostClassifier(verbose=0)),\n    ('reglog',LogisticRegression(C= 3,penalty= 'l2',solver= 'liblinear')),\n    ('xgb',XGBClassifier()),\n    ('SVM', SVC(probability=True,C=441,class_weight='balanced',coef0=0,shrinking=True,tol=0.01,gamma='scale',degree=1))\n]\n\nmodelos = {\n    'Regressão Logística': LogisticRegression(C= 3,penalty= 'l2',solver= 'liblinear'),\n    'KNN': KNeighborsClassifier(n_neighbors=16,p=1,weights='distance',metric='manhattan'),\n    'Decision Tree': DecisionTreeClassifier(criterion='entropy',max_depth=18,max_features=2,min_samples_leaf=4,min_samples_split=11),\n    'Random Forest': RandomForestClassifier(max_depth=20,max_features=1,min_samples_leaf=1,min_samples_split=4,n_estimators=1010,random_state=42),\n    'SVM': SVC(probability=True,C=441,class_weight='balanced',coef0=0,shrinking=True,tol=0.01,gamma='scale',degree=1),\n    'Naive Bayes': GaussianNB(),\n    'Gradient Boosting': GradientBoostingClassifier(),\n    'AdaBoost': AdaBoostClassifier(),\n    'Extra Trees': ExtraTreesClassifier(),\n    'Bagging': BaggingClassifier(),\n    'Perceptron': Perceptron(),\n    'MLP Classifier': MLPClassifier(max_iter=1000),\n    'QDA': QuadraticDiscriminantAnalysis(),\n    'XGBoost': XGBClassifier(),\n    'LightGBM': LGBMClassifier(verbosity=-1),\n    'CatBoost': CatBoostClassifier(verbose=0),\n    'Voting Classifier': VotingClassifier(estimators=[\n       ('rf', RandomForestClassifier(max_depth=20,max_features=1,min_samples_leaf=1,min_samples_split=4,n_estimators=1010,random_state=42)),\n       ('gb', GradientBoostingClassifier()),\n       ('et', ExtraTreesClassifier()),\n       ('catbost', CatBoostClassifier(verbose=0)),\n       ('reglog',LogisticRegression(C= 3,penalty= 'l2',solver= 'liblinear')),\n       ('xgb',XGBClassifier()),\n       ('SVM', SVC(probability=True,C=441,class_weight='balanced',coef0=0,shrinking=True,tol=0.01,gamma='scale',degree=1))\n    ], voting='soft'),\n    'Stacking': StackingClassifier(estimators=estimators, final_estimator=LogisticRegression())\n}\n\n# Cross-Validation e avaliação para cada modelo individual\nfor nome, modelo in modelos.items():\n    # Fit e Cross-Validation com 3 folds\n    modelo.fit(base_X_train, base_y_train)\n    cv_result = cross_val_score(modelo, base_X_train, base_y_train, cv=nc, scoring='accuracy')\n\n    # Previsões no conjunto de teste\n    #y_pred = modelo.predict(X_test)\n    y_pred = modelo.predict(teste_modelo)\n    \n    # Calcula métricas no conjunto de teste\n    acuracia = accuracy_score(teste_modelo_y, y_pred)\n    f1_positivo = f1_score(teste_modelo_y, y_pred, pos_label=1)\n    f1_negativo = f1_score(teste_modelo_y, y_pred, pos_label=0)\n\n    # Confusion matrix para extrair TP, TN, FP, FN\n    tn, fp, fn, tp = confusion_matrix(teste_modelo_y, y_pred).ravel()\n    erros = fp + fn\n\n    # Cria uma nova linha com os resultados\n    nova_linha = pd.DataFrame([{\n        'Modelo': nome,\n        'CV': cv_result.mean() * 100,\n        'DP': cv_result.std() * 100,\n        'Acurácia': acuracia * 100,\n        'F1 +': f1_positivo * 100,\n        'F1 -': f1_negativo * 100,\n        'VP': tp,\n        'VN': tn,\n        'FP': fp,\n        'FN': fn,\n        'Erros': erros\n    }])\n\n    # Adiciona a nova linha ao DataFrame\n    resultados_df = pd.concat([resultados_df, nova_linha], ignore_index=True)\n\nresultados_df = resultados_df.round(2)\n\n# Ordena o DataFrame 'resultados_df' pela coluna desejada (exemplo: 'Acurácia (X_test)') em ordem decrescente\nresultados_df = resultados_df.sort_values(by='Erros', ascending=True)\n\nfrom IPython.display import display, HTML\n\n# Converte o DataFrame para HTML com alinhamento centralizado\ndisplay(HTML(f\"<div style='display: flex; justify-content: center;'>{resultados_df.to_html(index=False)}</div>\"))\n","metadata":{"execution":{"iopub.status.busy":"2024-11-04T20:03:49.973721Z","iopub.execute_input":"2024-11-04T20:03:49.974078Z","iopub.status.idle":"2024-11-04T20:06:59.301797Z","shell.execute_reply.started":"2024-11-04T20:03:49.974042Z","shell.execute_reply":"2024-11-04T20:06:59.300392Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Importância das Variáveis por Permutação","metadata":{}},{"cell_type":"code","source":"\"\"\"\nfrom sklearn.inspection import permutation_importance\nfrom tqdm import tqdm\nimport pandas as pd\n\n#xgboost = XGBClassifier(objective='multi:softmax', num_class=2, random_state=42).fit(base_X_train, base_y_train)\nxgboost = XGBClassifier().fit(base_X_train, base_y_train)\ncatboost = CatBoostClassifier(verbose=0).fit(base_X_train, base_y_train)\n\nmodeloescolhido = catboost\nrepeticoes = 30\n\n# Função para exibir progresso ao fazer o permutation_importance\ndef permutation_importance_with_progress(model, X, y, n_repeats, random_state, n_jobs):\n    # Inicializa a barra de progresso com n_repeats iterações\n    with tqdm(total=n_repeats, desc=\"Calculando importância por permutação\", unit=\"iteração\") as pbar:\n\n        # Define uma função de callback que atualiza a barra de progresso após cada repetição\n        def update_progress_bar(_):\n            pbar.update(1)\n\n        # Executa o permutation_importance\n        perm_importance = permutation_importance(\n            model, X, y, n_repeats=n_repeats, random_state=random_state, n_jobs=n_jobs\n        )\n\n        # Atualiza a barra de progresso ao final\n        pbar.update(n_repeats)\n\n    return perm_importance\n\n# Chamar a função para calcular a importância com a barra de progresso\nperm_importance = permutation_importance_with_progress(modeloescolhido, X_test, y_test, n_repeats=repeticoes, random_state=42, n_jobs=-1)\n\n# Organizar os resultados em um DataFrame\nfeature_names = X_test.columns\nfeature_importances = pd.DataFrame({'Feature': feature_names, 'Importance': perm_importance.importances_mean})\nfeature_importances = feature_importances.sort_values(by='Importance', ascending=False)\n\n# Exibir as 10 features mais importantes\nprint(feature_importances)\n\"\"\"","metadata":{"execution":{"iopub.status.busy":"2024-11-04T20:06:59.308850Z","iopub.execute_input":"2024-11-04T20:06:59.309270Z","iopub.status.idle":"2024-11-04T20:06:59.317406Z","shell.execute_reply.started":"2024-11-04T20:06:59.309231Z","shell.execute_reply":"2024-11-04T20:06:59.316302Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.inspection import permutation_importance\n\nmodeloescolhido = xgboost\nrepeticoes = 30\n\n# Calcular a importância das features por permutação\nperm_importance = permutation_importance(modeloescolhido, X_test, y_test, n_repeats=repeticoes, random_state=42, n_jobs=-1)\n\n# Obter as importâncias médias e o desvio padrão das importâncias\nimportance_vals = perm_importance.importances_mean\nimportance_std = perm_importance.importances_std\nfeatures = X_test.columns\n\n# Certifique-se de que os arrays de importâncias e as features tenham o mesmo tamanho\nif len(importance_vals) != len(features):\n    raise ValueError(\"O número de importâncias não corresponde ao número de features. Verifique se X_test está correto.\")\n\n# Ordenar as importâncias em ordem decrescente\nindices = np.argsort(importance_vals)[::-1]\n\n# Escolher o número mínimo de features entre o solicitado (15) e o total disponível\ntop_n = min(15, len(importance_vals))\ntop_indices = indices[:top_n]\n\n# Plotar a importância das features mais importantes\nplt.figure(figsize=(10, 6))\nplt.title(\"Importância das Features por Permutação - XGBoost\")\nplt.barh(range(top_n), importance_vals[top_indices], xerr=importance_std[top_indices], align=\"center\", color='skyblue')\nplt.yticks(range(top_n), features[top_indices])\nplt.gca().invert_yaxis()  # Inverter o eixo Y para que a feature mais importante fique no topo\nplt.xlabel('Grau de Importância')\nplt.ylabel('Features')\nplt.tight_layout()\nplt.show()\n\"\"\"","metadata":{"execution":{"iopub.status.busy":"2024-11-04T20:06:59.318991Z","iopub.execute_input":"2024-11-04T20:06:59.319352Z","iopub.status.idle":"2024-11-04T20:06:59.336011Z","shell.execute_reply.started":"2024-11-04T20:06:59.319315Z","shell.execute_reply":"2024-11-04T20:06:59.334827Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Algorítmos - Teste Manual","metadata":{}},{"cell_type":"code","source":"\"\"\"from sklearn.ensemble import BaggingClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.datasets import make_classification\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\n\nbagging_model = BaggingClassifier()\n\n# Criando um modelo de Bagging com árvores de decisão\n#bagging_model = BaggingClassifier(\n#    base_estimator=RandomForestClassifier(max_depth=20,max_features=1,min_samples_leaf=1,min_samples_split=4,n_estimators=1010,random_state=42),\n#    n_estimators=100,  # Número de modelos (árvores) a serem treinados\n#    random_state=42,\n#    max_features=0.55,\n#    max_samples=0.65,\n#    bootstrap=1\n#)\n\n#{'base_estimator': 1, 'bootstrap': 1, 'max_features': 0.5514091541886079, 'max_samples': 0.6536918434137314}\n\n\n# Treinando o modelo\nbagging_model.fit(X, y)\n\n# Fazendo previsões e avaliando a acurácia\ny_pred = bagging_model.predict(teste_modelo)\nprint(\"Acurácia:\", accuracy_score(teste_modelo_y, y_pred))\n\"\"\"","metadata":{"execution":{"iopub.status.busy":"2024-11-04T20:06:59.337647Z","iopub.execute_input":"2024-11-04T20:06:59.338454Z","iopub.status.idle":"2024-11-04T20:06:59.355395Z","shell.execute_reply.started":"2024-11-04T20:06:59.338383Z","shell.execute_reply":"2024-11-04T20:06:59.354175Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"\n# Algoritmos de aprendizagem\nimport logging\nfrom sklearn.linear_model import LogisticRegression, Perceptron\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier, plot_tree\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.naive_bayes import BernoulliNB\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier, ExtraTreesClassifier, BaggingClassifier, StackingClassifier, VotingClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\nfrom catboost import CatBoostClassifier\nimport catboost as catboost\nimport logging\nfrom sklearn.model_selection import cross_val_score, GridSearchCV\n\nfrom sklearn.metrics import accuracy_score, f1_score, confusion_matrix\nimport pandas as pd\n\nbase_X_train = X.copy()\nbase_y_train = y.copy()\n\nmodelo_previsao = XGBClassifier().fit(base_X_train, base_y_train)\n\n#ensemble\nxgb_clf = XGBClassifier()\nxgb_clf.fit(base_X_train, base_y_train,eval_metric=[\"auc\", \"logloss\"],verbose=True)\n\nfrom lightgbm import LGBMClassifier\nimport lightgbm as lgb\n\nlgb_clf = LGBMClassifier()\nlgb_clf.fit(base_X_train, base_y_train)\n\nfrom catboost import CatBoostClassifier\nimport catboost as catboost\n\ncat_clf = CatBoostClassifier(verbose=0)\ncat_clf.fit(base_X_train, base_y_train)\n\nfrom sklearn.tree import DecisionTreeClassifier\ndecision_tree = DecisionTreeClassifier(max_depth=20,\n    criterion=\"entropy\",\n    min_samples_leaf=14,\n    min_samples_split=5) \ndecision_tree.fit(base_X_train, base_y_train)\n\nfrom sklearn.ensemble import RandomForestClassifier\nrandom_forest = RandomForestClassifier(n_estimators=200)\nrandom_forest.fit(base_X_train, base_y_train)\n\nfrom sklearn.linear_model import LogisticRegression\nreglog = LogisticRegression(max_iter=1000).fit(base_X_train, base_y_train)\n\"\"\"","metadata":{"execution":{"iopub.status.busy":"2024-11-04T20:06:59.357031Z","iopub.execute_input":"2024-11-04T20:06:59.357985Z","iopub.status.idle":"2024-11-04T20:06:59.371967Z","shell.execute_reply.started":"2024-11-04T20:06:59.357932Z","shell.execute_reply":"2024-11-04T20:06:59.370833Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Submeter","metadata":{}},{"cell_type":"code","source":"base_X_train = X.copy()\nbase_y_train = y.copy()\n\"\"\"\nestimators = [\n    ('rf', RandomForestClassifier(max_depth=20,max_features=1,min_samples_leaf=1,min_samples_split=4,n_estimators=1010,random_state=42)),\n    ('gb', GradientBoostingClassifier()),\n    ('et', ExtraTreesClassifier()),\n    ('catbost', CatBoostClassifier(verbose=0)),\n    ('reglog',LogisticRegression(C= 3,penalty= 'l2',solver= 'liblinear')),\n    ('xgb',XGBClassifier()),\n    ('SVM', SVC(probability=True,C=441,class_weight='balanced',coef0=0,shrinking=True,tol=0.01,gamma='scale',degree=1))\n]\n\"\"\"\n#stacking = StackingClassifier(estimators=estimators, final_estimator=RandomForestClassifier(max_depth=20,max_features=1,min_samples_leaf=1,min_samples_split=4,n_estimators=1010,random_state=42)).fit(base_X_train, base_y_train)\n\n#from sklearn.linear_model import LogisticRegression\n#reglog = LogisticRegression(max_iter=1000).fit(base_X_train, base_y_train)\n\n#from catboost import CatBoostClassifier\n#import catboost as catboost\n#catboost = CatBoostClassifier(verbose=0).fit(base_X_train, base_y_train)\n\ndecisiontree = DecisionTreeClassifier(criterion='entropy',max_depth=18,max_features=2,min_samples_leaf=4,min_samples_split=11).fit(base_X_train, base_y_train)\n\n#teste_modelo = pd.get_dummies(teste, drop_first=True)\n#teste_modelo = teste_modelo.reindex(columns=X.columns, fill_value=False)\n\nteste_previsao['Survived'] = decisiontree.predict(teste_modelo)\n\nresultado = teste_previsao[['PassengerId', 'Survived']]\nresultado['Survived'] = resultado['Survived'].astype(int)\n#resultado.loc[:, 'Survived'] = resultado['Survived'].astype(int)\n\n#print(resultado)\n\n#resultado.isnull().sum()\n\nresultado.to_csv('submission.csv', index=False)\n\n#from google.colab import files\n#files.download('submission.csv')\n\n#submit.to_csv(filename, index=False)\n#print(resultado)\n","metadata":{"execution":{"iopub.status.busy":"2024-11-04T20:06:59.373401Z","iopub.execute_input":"2024-11-04T20:06:59.373799Z","iopub.status.idle":"2024-11-04T20:06:59.398068Z","shell.execute_reply.started":"2024-11-04T20:06:59.373760Z","shell.execute_reply":"2024-11-04T20:06:59.396558Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Verificar Resultado","metadata":{}},{"cell_type":"code","source":"\"\"\"\ncomparacao = pd.merge(resultado, respostacorreta, on='PassengerId', suffixes=('_predito', '_correto'))\n\n# Calcular o número de acertos\nacertos = (comparacao['Survived_predito'] == comparacao['Survived_correto']).sum()\n\n# Calcular a porcentagem de acertos\nporcentagem_acertos = (acertos / len(comparacao)) * 100\n\n# Exibir os resultados\nprint(f\"Número de acertos: {acertos}\")\nprint(f\"Porcentagem de acertos: {porcentagem_acertos:.2f}%\")\n\n#print(comparacao)\n#print(len(comparacao))\n\"\"\"","metadata":{"execution":{"iopub.status.busy":"2024-11-04T20:06:59.399562Z","iopub.execute_input":"2024-11-04T20:06:59.400012Z","iopub.status.idle":"2024-11-04T20:06:59.407977Z","shell.execute_reply.started":"2024-11-04T20:06:59.399969Z","shell.execute_reply":"2024-11-04T20:06:59.406876Z"},"trusted":true},"execution_count":null,"outputs":[]}]}